{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "\n",
    "\n",
    "class cfg:\n",
    "    pretrained_checkpoint = 'microsoft/deberta-base'  # microsoft/deberta-large /  microsoft/deberta-v3-large\n",
    "    model_dir = \"/home/xm/workspace/output/1002\" # 模型权重文件路径\n",
    "    data_path = \"/home/xm/workspace/nbme-score-clinical-patient-notes/train_pl_all.pkl\" # 数据路径\n",
    "    batch_size = 128\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    '''\n",
    "    保存json文件\n",
    "    '''\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "pl_df = pd.read_pickle(cfg.data_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_checkpoint, trim_offsets=False)  # trim_offsets==False 删除因offsets造成的空白token\n",
    "pl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet\n",
    "def prepare_input(tokenizer, text, feature_text):\n",
    "    '''\n",
    "    构造 input 数据\n",
    "    '''\n",
    "    inputs = tokenizer(text, feature_text, #note and feature\n",
    "                       add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                       return_offsets_mapping=False # 将每个tokens映射回原始文本character级别的位置。\n",
    "                      )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "class NBMEDatasetInfer(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_texts = df['feature_text'].values # feature_text\n",
    "        self.pn_historys = df['pn_history'].values  # notes_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts) # 样本数\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.tokenizer,\n",
    "                               self.pn_historys[item],\n",
    "                               self.feature_texts[item]\n",
    "                              )\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True) # AutoConfig\n",
    "        self.backbone = AutoModel.from_pretrained(checkpoint) # AutoModel\n",
    "        self.dropout = nn.Dropout(0.1) # Dropout\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1) #MLP\n",
    "        self._init_weights(self.classifier) # 初始化 \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Linear 层\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Embedding 层 \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)  # initializer_range: 0.02\n",
    "            if module.padding_idx is not None:\n",
    "                # padding部分置零\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            # Normalization层 \n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        #inputs(dict)\n",
    "        #    input_ids, token_type_ids, attention_mask, label\n",
    "        outputs = self.backbone(**{k: v for k, v in inputs.items() if k != 'label'})\n",
    "        # outputs: [last_hidden_state], last_hidden_state: [bs, seq_len, hidden_size]\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(self.dropout(sequence_output)) #获得 preds\n",
    "        loss = None\n",
    "        if 'label' in inputs:\n",
    "            # 计算loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            loss = loss_fct(logits.view(-1, 1), inputs['label'].view(-1, 1).float())\n",
    "            loss = torch.masked_select(loss, inputs['label'].view(-1, 1) != -100).mean()\n",
    "        # 返回值\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, # loss\n",
    "            logits=logits, # logits\n",
    "        )\n",
    "        \n",
    "model = NBMEModel(cfg.pretrained_checkpoint).cuda() # 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_logits(texts, predictions, tokenizer):\n",
    "    '''\n",
    "    获得每个char级的预测概率值\n",
    "    texts: 原始notes文本数据(会重复，notes对应多个features)\n",
    "    predictions: token级预测概率值\n",
    "    '''\n",
    "    results = [np.zeros(len(t)) for t in texts] # 输出列表 [[0,0,0],[0,0,0,0]]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, # note\n",
    "                            add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                            return_offsets_mapping=True # 将每个tokens映射回原始文本char级别的位置。\n",
    "                           )\n",
    "        offset_mappings = encoded['offset_mapping']\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset_mappings, prediction)):\n",
    "            start, end = offset_mapping\n",
    "            results[i][start:end] = pred # char级填上logits\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # {id: char_logits}\n",
    "for fold in range(5): \n",
    "    pl_df_fold = pl_df[pl_df['fold'] == fold].reset_index(drop=True) # 全部数据fold\n",
    "    pl_dataset_fold = NBMEDatasetInfer(tokenizer, pl_df_fold) # 创建Dataset\n",
    "    dataset_len = len(pl_dataset_fold) # 样本数\n",
    "    maxlen = max([len(x['input_ids']) for x in pl_dataset_fold]) # 最长样本的len作为maxlen\n",
    "    # 创建DataLoader\n",
    "    dataloader = DataLoader(pl_dataset_fold, batch_size=cfg.batch_size, shuffle=False, \n",
    "                            collate_fn=DataCollatorForTokenClassification(tokenizer),\n",
    "                            num_workers=4,\n",
    "                            pin_memory=False)\n",
    "    \n",
    "    # if fold in [3,4]:\n",
    "    #     fold = 0\n",
    "    model.load_state_dict(torch.load(os.path.join(cfg.model_dir, f'{fold}.pt'))) # 载入模型权重\n",
    "    model.eval() # 评估模式\n",
    "    preds = []\n",
    "    for b in tqdm(dataloader, total=dataset_len // cfg.batch_size + 1):\n",
    "        b = {k: v.cuda() for k, v in b.items()} # batch\n",
    "        pred = model(**b).logits.squeeze()  # [bs, maxlen, 1]\n",
    "        pred = F.pad(input=pred, pad=(0, maxlen - pred.shape[1]), mode='constant', value=-100).cpu().numpy() # pad满maxlen，填充值-100 \n",
    "        preds.append(pred) \n",
    "    preds = np.concatenate(preds, axis=0)   # 所有样本的预测值 # [n, maxlen]\n",
    "    char_logits = get_char_logits(pl_df_fold['pn_history'].values, preds, tokenizer) # 获得char级的预测概率值\n",
    "    results.update({k: v for k, v in zip(pl_df_fold['id'], char_logits)})\n",
    "    \n",
    "print(f'{len(results)} results')\n",
    "# results 字典 样本id : 样本char级的预测概率值\n",
    "save_pickle(results, os.path.join(cfg.model_dir, 'pl_logits.pkl')) # 保存模型"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e16852016d24c6c850171da06836ee6dec9d765a18cae5c88840c842c8f803b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
