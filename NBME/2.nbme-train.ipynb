{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import uuid\n",
    "import datetime\n",
    "import json\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoTokenizer, DebertaV2Tokenizer\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class cfg:\n",
    "    exp_id = \"1002\" # 实验ID\n",
    "    seed = 42 # 随机种子\n",
    "    data_path = \"/home/xm/workspace/nbme-score-clinical-patient-notes/train_processed.pkl\" # \n",
    "    pretrained_checkpoint = '/home/xm/workspace/output/1001/checkpoint-7908'\n",
    "    lr = 1e-5\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    save_total_limit = 2 # 最多checkpoint的数量\n",
    "    fold = 5\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    '''\n",
    "    设置随机种子\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "\n",
    "def save_json(obj, path):\n",
    "    '''\n",
    "    保存json文件\n",
    "    '''\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=4)\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    '''\n",
    "    保存json文件\n",
    "    '''\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(cfg.data_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained_checkpoint, trim_offsets=False)  # trim_offsets==False 删除因offsets造成的空白token\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(tokenizer, text, feature_text):\n",
    "    '''\n",
    "    构造 input 数据\n",
    "    '''\n",
    "    inputs = tokenizer(text, #note\n",
    "                       feature_text, # feature\n",
    "                       add_special_tokens=True,# 加入特殊token 如[CLS]，[SEP] \n",
    "                       return_offsets_mapping=False # 将每个tokens映射回原始文本character级别的位置。\n",
    "                      )\n",
    "    return inputs\n",
    "\n",
    "def create_label(tokenizer, text, feature_text, annotation_length, location_list):\n",
    "    '''\n",
    "    构造 label 数据\n",
    "    '''\n",
    "    encoded = tokenizer(text, feature_text, #note and features\n",
    "                        add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                        return_offsets_mapping=True # 将每个tokens映射回原始文本character级别的位置，[SEP]后从0开始\n",
    "                       )\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    # encoded.sequence_ids() 会返回 [CLS] 0*notes [SEP]features[SEP], notes部分为0， feature部分为1\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # 非notes部分的全部索引值list\n",
    "    # 创建label list，非notes部分设置为-100\n",
    "    label = np.zeros(len(offset_mapping), dtype=int)\n",
    "    label[ignore_idxes] = -100\n",
    "    \n",
    "    if annotation_length != 0: # 有annotation的样本\n",
    "        for location in location_list: \n",
    "            for loc in [s.split() for s in location.split(';')]: # 循环每一个annotation\n",
    "                start_idx = -1; end_idx = -1 # token-level 位置,待映射\n",
    "                start, end = int(loc[0]), int(loc[1]) # char-level 位置\n",
    "                \n",
    "                # 用 char-level 位置 映射到 token-level 位置\n",
    "                for idx in range(len(offset_mapping)): # 循环每一个token\n",
    "                    \n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # \n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                if start_idx == -1: # 特例:当feature是最后一个词\n",
    "                    start_idx = end_idx - 1\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # 填充label列表\n",
    "                    \n",
    "    # label like:    \n",
    "    # array([-100,    0,    0,     0,    0,    0,    0,    0,    0,    \n",
    "    #         0,      1,    1,     1,    0,    0, -100, -100, -100,])\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "class NBMEDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.feature_texts = df['feature_text'].values # feature_text\n",
    "        self.pn_historys = df['pn_history'].values  # notes_text\n",
    "        self.annotation_lengths = df['annotation_length'].values # annotation 数量\n",
    "        self.locations = df['location'].values # annotation location\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts) # 长度\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.tokenizer,\n",
    "                               self.pn_historys[item],\n",
    "                               self.feature_texts[item]\n",
    "                              )\n",
    "        \n",
    "        label = create_label(self.tokenizer,\n",
    "                             self.pn_historys[item],\n",
    "                             self.feature_texts[item],\n",
    "                             self.annotation_lengths[item],\n",
    "                             self.locations[item]\n",
    "                            )\n",
    "        \n",
    "        return {**inputs, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, checkpoint):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True) # AutoConfig\n",
    "        self.backbone = AutoModel.from_pretrained(checkpoint) # AutoModel\n",
    "        self.dropout = nn.Dropout(0.1) # Dropout\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1) #MLP\n",
    "        self._init_weights(self.classifier) # 初始化 \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Linear 层\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Embedding 层 \n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)  # initializer_range: 0.02\n",
    "            if module.padding_idx is not None:\n",
    "                # padding部分置零\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            # Normalization层 \n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        #inputs(dict)\n",
    "        #    input_ids, token_type_ids, attention_mask, label\n",
    "        outputs = self.backbone(**{k: v for k, v in inputs.items() if k != 'label'})\n",
    "        # outputs: [last_hidden_state], last_hidden_state: [bs, seq_len, hidden_size]\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(self.dropout(sequence_output)) #获得 preds\n",
    "        loss = None\n",
    "        if 'label' in inputs:\n",
    "            # 计算loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            loss = loss_fct(logits.view(-1, 1), inputs['label'].view(-1, 1).float())\n",
    "            loss = torch.masked_select(loss, inputs['label'].view(-1, 1) != -100).mean()\n",
    "        # 返回值\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss, # loss\n",
    "            logits=logits, # logits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    将spans转换为二元数组，表明每个char是否在span中。\n",
    "    spans (list of lists of two ints).\n",
    "    Returns: np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1 # span部分填1\n",
    "    return binary\n",
    "\n",
    "def get_score(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of list of lists of two ints): Prediction spans.\n",
    "        truths (list of list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            # pred和truth的span都为空则跳过\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0) # 获取最大的span值\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "        \n",
    "        # bin_preds  (list of lists of ints(0 ro 1)): Predictions.\n",
    "        # bin_truths (list of lists of ints(0 ro 1)): Ground truths.\n",
    "        preds = np.concatenate(bin_preds)\n",
    "        truths = np.concatenate(bin_truths)\n",
    "        \n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    '''\n",
    "    计算验证集分数\n",
    "    \n",
    "    eval_pred:\n",
    "    logits [n, max_len, 1] / [2860, 417, 1], 填充值为-100\n",
    "    labels [n, max_len]    / [2860, 417]\n",
    "    '''\n",
    "\n",
    "    logits, labels = eval_pred # 验证集预测\n",
    "    logits = logits.reshape(logits.shape[0], -1) # [n, max_len, 1]  to [n, max_len]\n",
    "    assert logits.shape == labels.shape\n",
    "    assert len(logits.shape) == 2\n",
    "    predictions = (logits > 0).astype(int) #logits数组二元化为 0和1\n",
    "    predictions_masked = []\n",
    "    labels_masked = []\n",
    "    for i in range(logits.shape[0]):\n",
    "        for j in range(logits.shape[1]):\n",
    "            if labels[i][j] != -100:\n",
    "                predictions_masked.append(predictions[i][j])\n",
    "                labels_masked.append(labels[i][j])\n",
    "    return {\n",
    "        'nbme_f1': f1_score(labels_masked, predictions_masked)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_char_logits(texts, predictions, tokenizer):\n",
    "    '''\n",
    "    获得每个char上的预测概率值\n",
    "    texts: 原始notes文本数据(会重复，notes对应多个features)\n",
    "    predictions: token级预测概率值\n",
    "    '''\n",
    "    results = [np.zeros(len(t)) for t in texts] # 输出列表 [[0,0,0],[0,0,0,0]]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, # note\n",
    "                            add_special_tokens=True, # 加入特殊token 如[CLS]，[SEP] \n",
    "                            return_offsets_mapping=True # 将每个tokens映射回原始文本char级别的位置。\n",
    "                           )\n",
    "        offset_mappings = encoded['offset_mapping']\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset_mappings, prediction)):\n",
    "            start, end = offset_mapping\n",
    "            results[i][start:end] = pred # char级填上logits\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def my_get_results(char_logits, texts, th=0):\n",
    "    '''\n",
    "    生成所有样本的span字符串 of list，同一样本的span用;隔开\n",
    "    '''\n",
    "    results = []\n",
    "    for i, char_prob in enumerate(char_logits): # 循环所有样本\n",
    "        result = np.where(char_prob > th)[0] # 大于阈值的索引值\n",
    "        # 根据数值是否连续进行分组\n",
    "        # result: array([  0,   1,  90,  91,  92,  93,  94,  95,  96,  97,  98, 628, 629, 630])\n",
    "        # to\n",
    "        # result: [[0, 1], [90, 91, 92, 93, 94, 95, 96, 97, 98], [628, 629, 630]]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        temp = []\n",
    "        for r in result:\n",
    "            s, e = min(r), max(r)\n",
    "            while texts[i][s] == ' ': # 去掉左侧空格\n",
    "                s += 1 \n",
    "            while texts[i][e] == ' ': # 去掉右侧空格\n",
    "                e -= 1\n",
    "            temp.append(f\"{s} {e+1}\")\n",
    "        result = temp\n",
    "        result = \";\".join(result) # 加入;后保存\n",
    "        results.append(result)\n",
    "        \n",
    "    #  results like ['0 5;64 72', '91 99', '128 134']\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    '''\n",
    "    span 字符串 转 list\n",
    "    from ['0 5;64 72', '91 99', '128 134']  \n",
    "    return [[[0, 5], [64, 72]], [[91, 99]], [[128, 134]]]\n",
    "    '''\n",
    "    predictions = []\n",
    "    for result in results: # 循环所有样本的span字符串\n",
    "        prediction = []\n",
    "        if result != \"\": # 非空span\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df):\n",
    "    '''\n",
    "    label 格式化\n",
    "    from [['0 5', '64 72'], ['91 99'], ['128 134']] \n",
    "    return [[[0, 5], [64, 72]], [[91, 99]], [[128, 134]]]\n",
    "    '''\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location'] # 获取df的原始location\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst) # 同一样本之间的span加入分号\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]: # 根据分号进行列表创建\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "oof_preds = {}\n",
    "for fold in range(cfg.fold):\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    name = f\"{cfg.exp_id}_fold{fold}\" # exp_name\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./output/{name}\", # output 路径\n",
    "        evaluation_strategy=\"steps\", # 评估策略 steps\n",
    "        save_strategy=\"steps\", # 保存策略 steps\n",
    "        learning_rate=cfg.lr,\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=cfg.batch_size,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        load_best_model_at_end=True, # 训练结束时保存best model\n",
    "        warmup_ratio=0.2, # 升温比例\n",
    "        fp16=True, # 混合精度\n",
    "        dataloader_num_workers=4, # cpu 线程数\n",
    "        group_by_length=True, # 使用动态padding 更快的训练\n",
    "        run_name=name, # 实验ID\n",
    "        metric_for_best_model=\"nbme_f1\", # 评价指标 f1\n",
    "        save_total_limit=2, # 最多checkpoint的数量\n",
    "        label_names=['label'], # label 的列名\n",
    "        seed=cfg.seed, # 随机种子\n",
    "    )\n",
    "    model = NBMEModel(cfg.pretrained_checkpoint) # 创建模型\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model, # 模型\n",
    "        args,  # 超参数\n",
    "        train_dataset=NBMEDataset(tokenizer, train_df),  # train 数据集\n",
    "        eval_dataset=NBMEDataset(tokenizer, val_df), # valid 数据集\n",
    "        tokenizer=tokenizer, # tokenizer\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer), # 数据整理器\n",
    "        compute_metrics=compute_metrics # 计算评估分数\n",
    "    )\n",
    "    trainer.train() # 开始训练\n",
    "    \n",
    "    # 验证评估\n",
    "    predictions = trainer.predict(NBMEDataset(tokenizer, val_df)).predictions  # [n, maxlen, 1]\n",
    "    predictions = predictions.reshape(len(val_df), -1) # [n, maxlen]\n",
    "    char_logits = get_char_logits(val_df['pn_history'].values, predictions, tokenizer) # 每个样本char级的预测概率值\n",
    "    oof_preds.update({k: v for k, v in zip(val_df['id'], char_logits)}) # 字典 样本id : 样本char级的预测概率值\n",
    "    results = my_get_results(char_logits, val_df['pn_history'].values) # 生成所有样本的span字符串 of list\n",
    "    preds = get_predictions(results) # span 字符串 转 list\n",
    "    scores.append(get_score(create_labels_for_scoring(val_df), preds)) # 计算验证集score\n",
    "    print(f'fold {fold} score: {scores[-1]}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    Path(f\"./output/{cfg.exp_id}/\").mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"./output/{cfg.exp_id}/{fold}.pt\")\n",
    "    # shutil.rmtree(f\"./output/{name}\")  # 删除中间结果\n",
    "\n",
    "# 保存config和oof score\n",
    "print(f'cv score: {np.mean(scores)}')\n",
    "save_json({**vars(cfg), 'score': np.mean(scores)}, f\"./output/{cfg.exp_id}/config.json\")\n",
    "save_pickle(oof_preds, f\"./output/{cfg.exp_id}/oof.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e16852016d24c6c850171da06836ee6dec9d765a18cae5c88840c842c8f803b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
